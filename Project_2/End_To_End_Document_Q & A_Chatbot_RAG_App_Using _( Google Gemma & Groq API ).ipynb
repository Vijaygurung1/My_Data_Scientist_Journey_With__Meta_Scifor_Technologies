{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894f6325-df25-4b7e-9188-3e82b9410181",
   "metadata": {},
   "source": [
    "# Project Title: [ End-to-End Document Q & A Chatbot Using Google Gemma Open-Source Models And Groq API ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5cc251-3a65-4c6f-b08c-e2b07e6f080c",
   "metadata": {},
   "source": [
    "- This project demonstrates how to build a scalable, high-performance document Q&A system using cutting-edge open-source models and technology from \" Google and Groq \"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d260e2-5d23-4cf3-a0e1-338b70515d30",
   "metadata": {},
   "source": [
    "# Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699f2a5-7eee-49b6-9665-4829ba32b9ed",
   "metadata": {},
   "source": [
    "- This project aims to develop a document Q&A chatbot leveraging \"Google’s Gemma\" open-source language models and \"Groq’s\" high-speed inferencing engine. \n",
    "\n",
    "- The goal is to create an efficient and responsive system that can handle large language model (LLM) tasks in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e61b30-3fe6-4d61-88b5-5c3d8e7b0768",
   "metadata": {},
   "source": [
    "# 1.Gemma Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4193a0-5d39-4bf0-89ca-09f622cecc04",
   "metadata": {},
   "source": [
    "\"Gemma\" is a family of lightweight, open-source models derived from the same research used to build \"Google's Gemini models\".\n",
    "\n",
    "- Multiple Gemma variants exist, each tailored to different use cases\n",
    "\n",
    "- ( a )  Gemma 2 – ( which was recently launched )\n",
    "- ( b ) Gemma 1 \n",
    "- ( C ) Recurrent Gemma - ( For to improve memory efficiency ). \n",
    "- ( d ) Pali Gemma -  ( This specifically open vision language model)\n",
    "-(e )  Code Gemma -   ( Lets say if we really want to work with model which will be able to provide with us a amazing code assistance \n",
    "                    then we specifically used the Code Gemma ).\n",
    "\n",
    "\n",
    "- In this project, the [ \"Gemma-1\" ] model is used for practical implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7db748-3fec-441b-9ea2-e12fbffd3354",
   "metadata": {},
   "source": [
    "# 2.Groq Engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e1fde-be51-4321-be9a-48477da2146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "- \"Groq Engine\" is a cutting-edge inferencing platform designed to provide faster inference times than traditional GPUs.\n",
    "- It employs a Language Processing Unit (LPU), which overcomes the limitations of GPUs by improving compute density and memory bandwidth, \n",
    "-  significantly speeding up LLM tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc0d0b-4e91-42e5-aa09-2cac29645ec5",
   "metadata": {},
   "source": [
    "# About \"GROQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448d0db-a93f-4bef-ba9c-e3b64bc567fe",
   "metadata": {},
   "source": [
    "- Now with the help of this Groq what particular inferencing engine what so special about this “Groq”\n",
    "- Go lets read about the [ “Groq” ] \n",
    "\n",
    "- Definition :  Groq is on a mission to set the standard for “GenAI” inference Speed, helping real-time AI applications come to life today.\n",
    "\n",
    "- One problem is with working with this LLMs is Specifically with respect to inferencing how quickly we are able to get the response.\n",
    "    \n",
    "- If I will consider this “Groq” Platform, it uses something called as – LPU Inferencing Engine.\n",
    "    \n",
    "- LPU Inferencing Engine is nothing but LPU Stand for – Language Processing unit. It is a new type of end to end processing unit system.\n",
    "\n",
    "- That provide the faster inference for computationally intensive application with a sequential component to them such as AI language application.\n",
    "\n",
    "- What is the main this of LPU over here is that, it is pretty much faster for the Inferencing purpose, it is much more faster then GPUs.\n",
    "                                                                                                      \n",
    "- Question :-  Why it is so much faster then the GPU for LLM and GenAI?\n",
    "                                                    \n",
    "- Answer :\n",
    "                                                    \n",
    "- LPU is designed to overcome the two LLM bottlenecks compute density and memory bandwidth.\n",
    "                                                    \n",
    "- LPU has a great compute capacity than a GPU and a CPU in regards to LLM this reduces the amounts of time per word calculated. Allowing sequences of text to be generated much faster. \n",
    "                                            Additionally, eliminating external memory bottlenecks enables the LPU Inference Engine to deliver orders of magnitude better performance on LLMs compared to GPUs.\n",
    "\n",
    "- That is the region why LPU is very much important, there is  also a research paper  , we can refer if we want to go deep dive into it.\n",
    "\n",
    "- If we specifically called with respect to “Groq” it provides us API and if we see top right corner, this platform has almost every open sources model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa62ca-b5a0-4436-9fea-dce2f9aa67ee",
   "metadata": {},
   "source": [
    "# 3.Technology Stack:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845211c-03fe-4dfa-9e56-76c351ec7021",
   "metadata": {},
   "source": [
    "- [ LangChain ] : Used for setting up the Q&A framework.\n",
    "\n",
    "- [ FAISS ] : A vector store from Meta used for embedding and retrieving information from documents.\n",
    "\n",
    "- [ Streamlit ] : Used for building the interactive chatbot UI.\n",
    "    \n",
    "- [ PyPDF ] : For reading and processing PDF documents.\n",
    "    \n",
    "- [ Google Generative AI Embeddings ] : Used for embedding text into vectors.\n",
    "    \n",
    "- [ Groq Cloud ] : For deploying and running the model on Groq's LPU engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faabefb-5195-4809-9ca6-999b067d9b0c",
   "metadata": {},
   "source": [
    "# 4.Project Workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c00ae-812a-47da-8d54-3fc2b1d405e0",
   "metadata": {},
   "source": [
    "# ( A ) Environment Setup:\n",
    "\n",
    "- Create a Python virtual environment.Named it as a - [ GEMMA ]\n",
    "    \n",
    "- Install required libraries listed below inside a [ requirements.txt ] file\n",
    "\n",
    "\n",
    "- Faiss-cpu\n",
    "- Groq\n",
    "- Langchain-groq\n",
    "- PyPDF2\n",
    "- Langchain_google_genai\n",
    "- Langchain\n",
    "- Streamlit\n",
    "- Langchain_community\n",
    "- Python-dotenv\n",
    "- Pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd84814-02de-4f2d-98c7-d72c97d6d03a",
   "metadata": {},
   "source": [
    "# ( B ) Embedding and Data Ingestion:\n",
    "\n",
    "- Load PDF documents from a local directory.\n",
    "\n",
    "- Convert the text from these documents into chunks and embed them using \"Google Generative AI Embeddings\".\n",
    "\n",
    "- Store the embedded text in \"FAISS\", a vector store optimized for \"semantic search\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2218dd-3099-48a2-b10e-c13a0af4e0cf",
   "metadata": {},
   "source": [
    "# (C ) Q&A System:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a0402e-5c8f-4f16-afcd-721237b5b761",
   "metadata": {},
   "source": [
    "- Utilize LangChain to build the document Q&A system.\n",
    "\n",
    "- Define custom prompt templates to guide the chatbot's responses based on the document context.\n",
    "\n",
    "- Implement the chatbot using the Groq inferencing engine with the [ Gemma-7b-it model ], which handles the question-answering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44017865-0132-4495-9103-33073ddf26d3",
   "metadata": {},
   "source": [
    "# ( D ) Deployment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ebe81-aa6d-43ad-8558-c2cf2f95b184",
   "metadata": {},
   "source": [
    "- The chatbot is deployed using Streamlit, providing an interactive interface for users to input questions and receive answers \n",
    "    based on the provided documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8f4bb-3700-4a48-89ef-367d267bf9c3",
   "metadata": {},
   "source": [
    "# 5.Key Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ebd2b-2e3d-41f8-9dab-bf76b549ca43",
   "metadata": {},
   "source": [
    "- Fast Inference : The project leverages Groq's LPU engine for faster inference times, making the Q&A system highly responsive.\n",
    "\n",
    "- Scalable Models : The integration of Gemma models allows for flexibility in scaling up to larger models ( For Example : Gemma-7b) for more complex tasks.\n",
    "\n",
    "- Document-Based Q&A : The system efficiently handles documents, extracts relevant information, and provides accurate answers to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce88a7-d37a-4487-a3ca-d750f58346b4",
   "metadata": {},
   "source": [
    "# Below is the code with detailed explanations, line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f0b73-df5e-4987-8be4-bfdc1ff6e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "##Load the GROQ and Google API KEY from the .env file\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "st.title(\"Google_Gemma_Model : Document_Q&A\")\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Gemma-7b-it\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the questions based on the provided context only.\n",
    "    Please provide the most accurate response based on the question.\n",
    "    <context>\n",
    "    {context}\n",
    "    <context>\n",
    "    Questions: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Function to initialize vector embeddings\n",
    "def vector_embedding():\n",
    "    if \"vectors\" not in st.session_state:\n",
    "        st.session_state.embeddings=GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "        st.session_state.loader=PyPDFDirectoryLoader(\"./us_census\") ## Data Ingestion\n",
    "        st.session_state.docs=st.session_state.loader.load() ## Document Loading\n",
    "        st.session_state.text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200) ## Chunk Creation\n",
    "        st.session_state.final_documents=st.session_state.text_splitter.split_documents(st.session_state.docs[:20]) #splitting\n",
    "        st.session_state.vectors=FAISS.from_documents(st.session_state.final_documents,st.session_state.embeddings) #vector OpenAI embeddings\n",
    "\n",
    "# Input for the user's question\n",
    "prompt1 = st.text_input(\"Enter Your Question From Documents\")\n",
    "\n",
    "# Button to initialize document embeddings\n",
    "\n",
    "if st.button(\"Initialize Document Embeddings\"):\n",
    "    vector_embedding()\n",
    "    st.success(\"Vector Store DB is Ready\")\n",
    "\n",
    "import time\n",
    "\n",
    "if prompt1:\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retriever = st.session_state.vectors.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    start = time.process_time()\n",
    "    response = retrieval_chain.invoke({'input': prompt1})\n",
    "    print(\"Response time :\", time.process_time() - start)\n",
    "    st.write(response['answer'])\n",
    "\n",
    "#With a streamlit expander\n",
    "\n",
    "    with st.expander(\"Document Similarity Search\"):\n",
    "        # Find the relevant chunks\n",
    "        for i, doc in enumerate(response[\"context\"]):\n",
    "            st.write(doc.page_content)\n",
    "            st.write(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27126727-d1ef-42fb-b9ff-e734448be77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959ccb5-7baa-462a-b9ad-f0ed948fa016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd7d15af-e679-4957-a0ea-42531829eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I am going to use “Groq” so, in Groq we have to chat groq. So, that we will be able to create a chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487c284-0b39-4247-91ea-4b4b0724c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6db3fbb3-15c5-49e4-b51c-2a2567d81ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once I probably read any documents, I should be able to convert that into chunks. So, for that I will be using from ( langchain.text_splitter ) and\n",
    "# [ import RecursiveCharacterTextSplitter ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7cbddc-4c23-4bd9-8468-1c5659d68550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b601fdc-5fe7-4ec7-800e-f76c0906f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Along with this I am using one more libraries called [ langchain.chains.combine_documents ] and\n",
    "# I am going to create [ import create_stuff_documents_chain  ]. So, in Langchain libraries, we use this create stuff documents chain for the\n",
    "# relevant documents and Q & A. This is will basically help us to set up the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122e4d7-c508-4b01-bae8-a92be061d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a160e-15c7-4d2b-98ed-b2e12f81c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will also going to use [ ChatPromptTemplate ]. So, that we will be able to create a our own custom prompt template. \n",
    "# So, here we are writing from [ langchain_core.prompts import ChatPromptTemplate ]. This is my vector store DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ec57b-ed51-4881-92e7-ae28f7b492cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c190a2-636a-4049-b4dc-984e8cb4f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, here we will be embedding our vector store DB that we are specifically going to use and for that we are going to import from the\n",
    "# [ langchain_community.vectorstores import FAISS ] .\n",
    "\n",
    "# “FAISS” is kind of a vector store that has been created, by Meta and internally it is basically used to store vectors and\n",
    "# it internally performs semantic search and similarities search to give us the result based on the information that I have asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce281c-7911-48dd-96c1-c0fea4e5fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6268dc-478e-4104-8527-2df15ca049d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I will also importing my libraries which is [ langchain_community.document_loaders ], we are using [ PyPDF ] directory\n",
    "# because our main aim is that we will be reading some PDF files that from our folder and then we will be reading all the documents and then, \n",
    "# we will be dividing chunk using this recursive character Text plater from langchain_community.document_loaders import PyPDFDirectoryLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869f26b9-09b1-4597-8f61-81d068c40850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, finally we also going to use one more library, that is our embedding technique for that we are going to import from \n",
    "# the [ langchain_google_genai ] because Google Already provide this, Google GenAI embedding which we can completely use it. \n",
    "# Its completely freely available, just by using the “GOOGLE_API_KEY”.\n",
    "# These  [ GoogleGenerativeAIEmbeddings ] will be responsible to converting my text, a chunks of text into the vectors.\n",
    "# This is my vector Embedding techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e477c3-c1f1-413a-8557-dd9c97691b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6180e1a9-95ca-41d0-8a91-2f57bd76aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next : Step code2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0baa9-f17c-4a35-a1b4-b17cd0ab6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now from dotenv we are going to import load_ underscore dotenv.\n",
    "# I am using this libraries region is that,so that we can actually go ahead and load all our environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80361c-ffc6-4ff4-8fee-3fa632561845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10cb90-cb43-4223-80f8-7a7e7d547545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next : Step code 3 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c0500-8414-44b6-a768-03c11dbaa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Load the GROQ and Google API KEY from the [ .env ] or environment variable file.\n",
    "# How do I load it, for that [ groq_api_key = os.getenv(\"GROQ_API_KEY\") ]\n",
    "# Here , I am using my Amazon Q as my code assistance over here.So, automatically I am able to get the suggestion. \n",
    "# So, the both below environment variable is created in my [ .env ] file or folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31eb654-3d37-4914-a599-6fb184a92387",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93a837-9c44-4fbb-a568-6d50b1ce3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next : Step code 4 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b0e62-a26c-4741-8350-e3d4ab140c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets st.title here I (\"Google_Gemma_Model : Document_Q&A\"). Here specifically we are using “Groq”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d62a91-fb12-4013-bbb2-590f10b86b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Google_Gemma_Model : Document_Q&A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55204be-f486-4638-8bcf-52c252b2dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I am just calling my LLM model, here I am going to use my “ChaGroq”, with respect to that, I am going to use my GROQ_API_KEY and \n",
    "# specifically say model_name, and here I am going to give my Model name, which is – [ Gemma-7b-it ].\n",
    "# This particular model, I am using it and with the help of GROQ_API_KEY, I will be able to call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5571c-3041-4691-8574-a04fe5a5f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Gemma-7b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358fb99-f7af-4d31-81c4-389e8a251d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next : Step code 5 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7dfe8-8328-4209-885b-743ee9564ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now here what we will do is, we will going to set up our Prompt template.To set up Prompt template, \n",
    "# I will be using this same [ ChatPromptTemplate.from_template ] & I am going to write these prompt as – \n",
    "\n",
    "# \"\"\"\n",
    "# Answer the questions based on the provided context only.\n",
    "# Please provide the most accurate response based on the question.\n",
    "# <context>\n",
    "# {context}    # The context will be this, from this particular context,\n",
    "# <context>\n",
    "# Questions: {input}   # It will going to take this particular input over here.\n",
    "# \"\"\"\n",
    "# By taking this all information, it is going to give us the entire information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a30178-7785-4542-a0c6-a1bbfd57d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Answer the questions based on the provided context only.\n",
    "Please provide the most accurate response based on the question.\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "Questions: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80174deb-e768-478b-adf6-d2feaf456cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next : Step code 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2db373-675b-4af4-abe7-d727170b7fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, now we are going to create a function which is called as Vector Embedding & this vector Embedding function, It is going to do is that.\n",
    "# We will be reading all the documents from our PDF Files.\n",
    "# From those PDF files, we also going to convert these into chunks then apply embeddings.\n",
    "# We are going to apply “GOOGLE_GENERTIVE_AI Embedding and then finally, we will be storing it in a vector store DB, that is called as “FAISS”. \n",
    "# We will going to keep this vector store DB, even in our session state so that we will be able to use it anywhere when it is required.\n",
    "# So 1st of all that, I am going to do over here is that, I am going to make sure that one type of PDFs, I should be able to upload over here. \n",
    "# I will create a new folder name it as a – [ us_census ]\n",
    "# I will save this folder with 4 PDF files inside it and save it inside my repository directory, where is \n",
    "# my all files stores + codes and every available. \n",
    "# This is my Dirctory inside my system – [ C:\\Users\\Public\\Music\\GEMMA> ] \n",
    "\n",
    "# This 4 PDF specifically we are going to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c8798-c614-4d4d-971d-98abdd0ef99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_embedding(): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0aa297-561d-445a-8e55-36152e6c2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now here, 1st vector store DB, that I am going to create in a variable that variable I will just go ahead and write it somewhere like this.\n",
    "\n",
    "# So, lets go and write it. \n",
    "# Next Line :\n",
    "# If vectors, since I also need to use session states, not in St.Session_States, So I will not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55083820-bbe6-4da1-8364-bbbf56a33bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"vectors\" not in st.session_state: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c4dc5-01ab-4afa-be65-e1802d397236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now here, 1st vector store DB, that I am going to create in a variable that variable I will just go ahead and write it somewhere like this.\n",
    "\n",
    "# So, lets go and write it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b32b90-02fc-4bae-af7a-821926b671a2",
   "metadata": {},
   "source": [
    "- Next : Line 1 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8871d-c48e-485a-8efb-0893d0a90dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If vectors, since I also need to use session states, not in St.Session_States, So I will not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01da5d-ef23-4f74-b4a1-42914a121c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"vectors\" not in st.session_state: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b30085-4271-4721-8d3e-b5f0731f1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, I will make code very simple, because I need to save everything in session states with respect to different different variables.\n",
    "\n",
    "# If the vector is not in the session states, 1st things I am going to write over here is that, \n",
    "# go ahead & write the st.session_state.embeddings, and 1st of all I am going to define my # embeddings for that, \n",
    "# I am going to use my [ GoogleGenerativeAIEmbeddings ] inside this I am going to use use my model, which is basically called as \n",
    "# [ models/embeddings-001 ]. This is the one of the model, which is available in Google. For the embedding purpose.\n",
    "\n",
    "# Now I am actually going to stay share in the form of the session states with this variable that is called as “Embeddings”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455c40b-12d8-4840-8f71-df1b5fd03e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.session_state.embeddings=GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac361173-fea8-4232-b605-9b8e07202568",
   "metadata": {},
   "source": [
    "- Next : Line 2 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696244eb-9f17-4e71-b7bf-af7a9808dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will write [ st.session_state.loader= here I am specifically use the PyPDF Directory and my folder name is where I have kept my all 4 PDF files.\n",
    "# Name is – [ us_census ].\n",
    "# This is basically my Data injection phase, once I read it I am storing entire loader in a session states with this particular variable name.\n",
    "# This is my basically “Data Ingetsion”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac3a96-5622-4566-95b6-b7d402a46159",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.session_state.loader=PyPDFDirectoryLoader(\"./us_census\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a648d658-1288-4fd6-b803-4bca1a7c2cbd",
   "metadata": {},
   "source": [
    "- Next : Line 3 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5574860-3126-4589-a3fe-d3e177d0cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# St.session_states.docs= & I am going to create another variable called as docs. Let write it st.session_underscore state.loader.load.\n",
    "# When we use this loader.load in short, it is going to load all the documents.This is basically loading all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e194b-3c3e-4a38-ab40-f69599201ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.session_state.docs=st.session_state.loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ee425-3b15-4651-be6d-b8033531d3cd",
   "metadata": {},
   "source": [
    "- Next: Line 4 –"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e104b8-a60e-4136-80da-370e2565d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to use this st.session_states.text_Splitter, because when we write the loader.load, we will specifically get all the docments.\n",
    "# Inside my particular variable Docs.\n",
    "# So, just by using this [ st.session_state.text_splitter ] This is my variable name. Here, we will be going to use  [ RecursiveCharacterTextSplitter ]\n",
    "# where in all these documents will be Splitted into chunks. Here we will going to use a chunk size of [ Chunk_size = 1000 and chunk_overlap = 200 ].\n",
    "# In short, these [ RecursiveCharacterTextSplitter ] function, which is going to take out the documents and Splits based on this Chunk_Size and\n",
    "# Chunk_overlap, Chunk_overlap is basically means there is a overlap of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b3129-a700-4195-bd1b-cfc6633227ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.session_state.text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3d15f-fa5e-401b-ab69-b2b803edabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, I will go ahead and write the st.session_State.  And  basically I am going to create my final_documents.\n",
    "# And lets use this text Splitter [ st.session_state.text_splitter.split_documents ] and here we are going to use entire docs\n",
    "# – [ st.session_state.docs[:20]) ]\n",
    "# That is the region, we are going have given the [  st.session_state.docs[:20]) ] \n",
    "# Region why we are saving this all in this session_States, because we should be able to use it anywhere that we required and finally after doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb7b97-7693-4a26-bccd-894a731dbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.session_state.final_documents=st.session_state.text_splitter.split_documents(st.session_state.docs[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a693c1-0c8f-4947-879e-c9d34fab23b6",
   "metadata": {},
   "source": [
    "- Next : Line 5 – "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39aea8-9aa7-474c-8c9c-4a226cc94a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, after doing this, we will finally create our vector_Store a Vector OpenAI Embeddings \n",
    "# st.session_state.vectors and I am going to use this “FAISS”. From documents and here I am going to use this – [ st.session_state.final_documents ]\n",
    "# and the Embeddings techniques it will be the same Embedding techniques that we have initial over here that is nothing,\n",
    "# but Google Generative_AI Embedding.\n",
    "# And this embedding – [ st.session_state.embeddings ] Will be responsible for the converting this all final documents into vectors, and\n",
    "# this “FAISS” will be responsible in storing all those embeddings into this particular vectors.\n",
    "# This is the Function that is probably doing all these things.\n",
    "# vector OpenAI embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223ae29-504d-4adb-a251-512ef3c00e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.session_state.vectors=FAISS.from_documents(st.session_state.final_documents,st.session_state.embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2f05a-9a38-4386-b4df-e5906a0e4d54",
   "metadata": {},
   "source": [
    "- Next : Step code 6 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634a77f-6a1c-4a19-9b7c-70bf53e320b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After close the function, Here I have crated the field, so lets write.\n",
    "# prompt1 and lets create the  st.text_input(\"Enter Your Question From Documents\"). \n",
    "# Here basically, what I want to asked from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096285e-4c7b-4777-b7de-8040e0309594",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = st.text_input(\"Enter Your Question From Documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837f84b-514e-46fe-8bc7-98f68796e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I am creating the Button, This Button will be responsible for on - Initialize Document Embeddings.\n",
    "#  This basically means, If I click this button then my entire process of this vector embedding should happened.\n",
    "# So, here what I am calling my vector embedding,\n",
    "#  when this entire embedding is created, I will just go-ahead and write, my vector store my vector DB is ready. \n",
    "# Because from these vector store, I am going to do is that, quey anything.\n",
    "# Before that we required this vector_states Variable, this vector DB should be there.\n",
    "# So, If I will click this below button, it automatically that vector embedding will created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4f7d9-61da-4bfb-8dc4-6077f0833112",
   "metadata": {},
   "outputs": [],
   "source": [
    "if st.button(\"Initialize Document Embeddings\"):\n",
    "    vector_embedding()\n",
    "    st.success(\"Vector Store DB is Ready\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad5667f-1d5e-47fd-8391-1dd75c462871",
   "metadata": {},
   "source": [
    "- Next : Step code 8 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a1ad8-7e95-40f7-8e26-5ae40857686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, lets try to create something with respect to the “time”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258eaa4-5c66-411f-892c-9e264990aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt1:\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retriever = st.session_state.vectors.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    start = time.process_time()\n",
    "    response = retrieval_chain.invoke({'input': prompt1})\n",
    "    print(\"Response time :\", time.process_time() - start)\n",
    "    st.write(response['answer'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69614aea-f3b3-4932-9880-babf54aba3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because, I am saying that, this is really really very fast, This “Goq_LPU” is really very fast.\n",
    "# So, lets record the “time” also, with that we will be able to understand the importance of it.\n",
    "# So, if prompt1 :\n",
    "# when basically, I am writing any text and press enter, I should take this particular input and create my document chain.For that,\n",
    "# I am using this [ create_stuff_documents_chain ]\n",
    "# inside this, there is 2 parameters will be given, one is (llm, prompt). The LLM model, I am using here is nothing but  - [ Gemma-7b-it ].\n",
    "# And the 2nd parameters, that I am using is basically nothing but – [ Prompt ].Both these things are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d950040-df56-4381-98a6-d622576374b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if prompt1:\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33deaa36-ba3b-44ed-a776-2f5e9ca9d427",
   "metadata": {},
   "source": [
    "- Next : Step 9 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0114c7-cd23-441b-b6cd-e88164df6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I will go-ahead and create a [ retriever ] and this is basically be [ st.session_state.vectors.as_retriever() ].\n",
    "# what is the main functionality of this particular retriever,\n",
    "# This vector is a vector database, now to retriever information from this vectors, vector database, with the help of this particular function\n",
    "# as retriever, it create a interface. So, whatever question basically we will ask through this interface, it will be able to take this \n",
    "# particular response and it will be able to give to the end user. That is the region we specifically used the “Retriver”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af1ad9-763b-46a8-be51-952af4608b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = st.session_state.vectors.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c84440-16c3-4d65-8403-0f67e4bb4f3f",
   "metadata": {},
   "source": [
    "- Next : Step 10 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d8bf4-8acf-4646-90f2-fa20ccae38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating this retriever, I really need to run this in the form of chain, where I have my retriever, where I have my documents chain.\n",
    "# Boths needs to be combined. So basically we are creating a retriever_chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ec4d9-f16f-4f78-b3ec-142bf655c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = create_retrieval_chain(retriever, document_chain) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd687d4-9704-40ec-bc8c-16edd9686540",
   "metadata": {},
   "source": [
    "- Next Step 11 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18fa0d-5beb-4083-8d7b-d32632e8e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I am going to start my time, I am going to say -  [   time.process_time() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45308a5b-a3ba-43d0-a695-b91fa3b892f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ccb27-f323-4289-9998-88001724147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, I will go ahead and write my response, which is nothing but it will be “Retriver_chain”\n",
    "# And, we will going to call the invoke function. Inside this Invoke function, we can keep our variable as ({'input': prompt1}) input  and\n",
    "# this will be equal to my prompt1.\n",
    "# Because I am going to sent, whatever input, I am giving in this particular prompt with respect to my question, it should be able to retrieve, \n",
    "# from this entire chain. Then finally I will get my response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fd63e-0139-4290-b502-5e04e9295ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({'input': prompt1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4939d9-04a2-4734-b042-cba094374ceb",
   "metadata": {},
   "source": [
    "- Next : Step 12 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4bb24-a89f-4115-828e-d0e3a5a718f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After get my entire response, basically I am going to do is display that entire response.In my streamlit APPs.\n",
    "# Note : \n",
    "# When this Gemma Model provide a response, it will also provide some kind of context in return.\n",
    "# I will try to display that content over here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcff1cc-f7f9-45a8-9ac6-01984aa08c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response time :\", time.process_time() - start)\n",
    "    st.write(response['answer'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901dcc0c-c48d-465e-a6f9-22dba8731dce",
   "metadata": {},
   "source": [
    "# Next : Step 13 :                       [  StreamLit  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa15865-5889-4c50-8f11-86825055ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This below is my final code for Streamlit, step by step explanation:\n",
    "# With a streamlit expander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fd1cb-75a4-41ee-90f1-0b3a3af808fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with st.expander(\"Document Similarity Search\"):\n",
    "        for i, doc in enumerate(response[\"context\"]):\n",
    "            st.write(doc.page_content)\n",
    "            st.write(\"--------------------------------\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d08632-38f7-4dda-b367-9a69054e4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here,this line creates an interactive expander in a Streamlit app. An expander is a collapsible container that can be expanded or \n",
    "# collapsed by the user. Initially, it shows as a heading labeled \"Document Similarity Search,\" and users can click to expand or collapse it.\n",
    "# Streamlit Context: The with statement here is used to create a context where all the code inside it will be executed only if the expander is \n",
    "# opened by the user. This is similar to how we would work with st.sidebar or st.form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a237f2-06ac-4fbb-a17f-1321cfb0faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with st.expander(\"Document Similarity Search\"):   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc9628-491b-49d2-8d2c-9fb28c56147c",
   "metadata": {},
   "source": [
    "- Next : Line 14 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9425533-35a2-4f58-9e15-087d961e683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is a for loop that iterates over the elements of response[\"context\"].\n",
    "# Enumerate function: The enumerate function adds a counter to the iteration. The variable I will represent the index (starting from 0) of each doc in\n",
    "# the response[\"context\"] list.\n",
    "# Response[\"context\"]: This is likely a list of documents or chunks of text that have been retrieved as part of a document similarity search.\n",
    "# Each doc represents a chunk or document that has been identified as similar, response of [\"context] through which I will be able to get \n",
    "# my entire context information. [i ], doc basically have the page content, which will get display.\n",
    "\n",
    "# The region I am using enumerate function over here is that, because there will be 2 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058fe59-0b9f-45b6-86fa-2623fa4cc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(response[\"context\"]): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36119b7d-1c32-4993-95cb-5b0d1ab33ef5",
   "metadata": {},
   "source": [
    "- Next Line 15 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9b64f-3b70-418d-9aac-fbb40f872acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line writes the content of each document (doc.page_content) to the Streamlit app. The st.write function can display text, markdown or\n",
    "# even more complex objects.\n",
    "# doc.page_content: This likely refers to the actual content (text) of the document or chunk that is being displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf3ec6-6188-4240-99e0-8a2cafcb31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.write(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19343a0f-51a2-42f9-99ef-c201ad62a7b1",
   "metadata": {},
   "source": [
    "- Next : Line 16 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832bfd8-a8fe-4473-b1e2-7c32919c1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line writes a visual separator (a row of dashes) between the different documents being displayed. This helps to visually distinguish between different chunks or\n",
    "# documents in the app.\n",
    "# Why a separator? After each document's content is displayed, a line of dashes is added to separate it from the next document,\n",
    "# making the output easier to read and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d3a96-44c5-4444-bff3-7fffaf4fd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.write(\"--------------------------------\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9eff7-8a7c-4dc7-84e4-3261238bfd30",
   "metadata": {},
   "source": [
    "# “ Terminal”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4543023-45e4-4b7d-a196-3dd3d8e1e3ed",
   "metadata": {},
   "source": [
    "- Next Line 17 :   [   Go to the “ Terminal” section to see the final outcome ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbc78c-a2e7-43ad-95fc-fd37e9170b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, Lets go-ahead and run this entire code to see my final outcome, in my \"Streamlit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8cc61-cb76-4be4-839f-38bc2ddbdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit run app.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf745b7-353e-4162-8870-456ea153b268",
   "metadata": {},
   "source": [
    "# Project_Deploye_on_The_Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef5f02-8a0c-4605-ab99-2fa2ca117f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we run this line of code, pop up open and in the browser we can see my final Apps has been Deployed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16233d-654c-432e-ae78-44b961df1034",
   "metadata": {},
   "source": [
    "- After successfully deploying my project, I searched for 2 titles from a single PDF file. Overall, I have 4 PDF files in my directory.\"\n",
    "\n",
    "- I have search these below 2 titles :\n",
    "\n",
    "1.\t[  WHAT IS HEALTH INSURANCE COVERAGE ]  \n",
    "2.\t[ WHAT IS DIFFRENCES IN THE UNINSURED RATE BY STATE IN 2022 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549e9a9-2ed0-4e84-83f9-4f3e62eb4852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
